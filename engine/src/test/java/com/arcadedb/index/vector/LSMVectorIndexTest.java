/*
 * Copyright Â© 2021-present Arcade Data Ltd (info@arcadedata.com)
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * SPDX-FileCopyrightText: 2021-present Arcade Data Ltd (info@arcadedata.com)
 * SPDX-License-Identifier: Apache-2.0
 */
package com.arcadedb.index.vector;

import com.arcadedb.GlobalConfiguration;
import com.arcadedb.TestHelper;
import com.arcadedb.database.DatabaseFactory;
import com.arcadedb.database.DatabaseInternal;
import com.arcadedb.database.RID;
import com.arcadedb.engine.PageId;
import com.arcadedb.index.IndexCursor;
import com.arcadedb.index.TypeIndex;
import com.arcadedb.schema.DocumentType;
import com.arcadedb.schema.Type;
import com.arcadedb.schema.TypeLSMVectorIndexBuilder;
import com.arcadedb.utility.Pair;

import org.junit.jupiter.api.Test;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Comparator;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Stream;

import static org.assertj.core.api.Assertions.assertThat;

/**
 * Tests for LSMVectorIndex using JVector.
 *
 * @author Luca Garulli (l.garulli@arcadedata.com)
 */
class LSMVectorIndexTest extends TestHelper {

  private static final int DIMENSIONS = 1024;

  @Test
  void createIndexViaSQLAndQuery() {
    database.transaction(() -> {
      // Create the schema
      database.command("sql", "CREATE VERTEX TYPE VectorVertex IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY VectorVertex.id IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY VectorVertex.title IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY VectorVertex.embedding IF NOT EXISTS ARRAY_OF_FLOATS");
      database.command("sql", "CREATE PROPERTY VectorVertex.category IF NOT EXISTS STRING");

      // Create the LSM_VECTOR index
      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON VectorVertex (embedding) LSM_VECTOR
          METADATA {
            "dimensions" : 1024,
            "similarity" : "COSINE",
            "maxConnections" : 16,
            "beamWidth" : 100,
            "addHierarchy": true
          }""");
    });

    // Verify the index was created
    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("VectorVertex[embedding]");
    assertThat(typeIndex).as("Index should be created").isNotNull();

    // Get one of the underlying LSMVectorIndex instances to verify configuration
    final LSMVectorIndex index = (LSMVectorIndex) typeIndex.getIndexesOnBuckets()[0];
    assertThat(index.getDimensions()).as("Dimensions should be " + DIMENSIONS).isEqualTo(DIMENSIONS);
    assertThat(index.getSimilarityFunction().name()).as("Similarity should be COSINE").isEqualTo("COSINE");
    assertThat(index.getMetadata().addHierarchy).as("addHierarchy should be true").isTrue();

    // Insert test data
    database.transaction(() -> {
      for (int i = 0; i < 100; i++) {
        final var doc = database.newVertex("VectorVertex");
        doc.set("id", "doc" + i);
        doc.set("title", "Document " + i);

        // Create a random DIMENSIONS-dimensional vector
        final float[] vector = new float[DIMENSIONS];
        for (int j = 0; j < DIMENSIONS; j++)
          vector[j] = (float) Math.random();

        doc.set("embedding", vector);
        doc.set("category", "category" + (i % 3));
        doc.save();
      }
    });

    // Query the index using TypeIndex
    database.transaction(() -> {
      final float[] queryVector = new float[DIMENSIONS];
      Arrays.fill(queryVector, 0.5f);

      final IndexCursor cursor = typeIndex.get(new Object[] { queryVector }, 10);

      int count = 0;
      while (cursor.hasNext()) {
        assertThat(cursor.next()).isNotNull();
        count++;
      }

      assertThat(count).as("Should find at least one result").isGreaterThan(0);
      assertThat(count).as("Should return at most 10 results").isLessThanOrEqualTo(10);
    });
  }

  @Test
  void createIndexProgrammatically() {
    database.transaction(() -> {
      // Create document type
      final DocumentType docType = database.getSchema().createDocumentType("VectorDoc");
      docType.createProperty("id", Type.STRING);
      docType.createProperty("embedding", Type.ARRAY_OF_FLOATS);

      // Create LSM_VECTOR index programmatically using unified API
      TypeLSMVectorIndexBuilder builder = database.getSchema()
          .buildTypeIndex("VectorDoc", new String[] { "embedding" }).withLSMVectorType();

      // Set common index properties
      builder.withIndexName("VectorDoc_embedding_idx");

      // Cast to LSMVectorIndexBuilder to access vector-specific methods
      builder.withDimensions(3);
      builder.withSimilarity("EUCLIDEAN");
      builder.withMaxConnections(8);
      builder.withBeamWidth(50);
      builder.withAddHierarchy(true);
      builder.create();
    });

    // Verify index was created
    // Note: TypeIndex is created with default name based on type and properties
    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("VectorDoc[embedding]");
    assertThat(typeIndex).isNotNull();

    // Get one of the underlying LSMVectorIndex instances to verify configuration
    final LSMVectorIndex index = (LSMVectorIndex) typeIndex.getIndexesOnBuckets()[0];
    assertThat(index.getDimensions()).isEqualTo(3);
    assertThat(index.getSimilarityFunction().name()).isEqualTo("EUCLIDEAN");
    assertThat(index.getMaxConnections()).isEqualTo(8);
    assertThat(index.getBeamWidth()).isEqualTo(50);

    // Insert and query
    database.transaction(() -> {
      for (int i = 0; i < 10; i++) {
        final var doc = database.newDocument("VectorDoc");
        doc.set("id", "doc" + i);
        doc.set("embedding", new float[] { (float) i, (float) i + 1, (float) i + 2 });
        doc.save();
      }
    });

    database.transaction(() -> {
      final float[] queryVector = { 5.0f, 6.0f, 7.0f };
      final IndexCursor cursor = typeIndex.get(new Object[] { queryVector }, DIMENSIONS);

      int count = 0;
      while (cursor.hasNext()) {
        count++;
        cursor.next();
      }

      assertThat(count).isGreaterThan(0);
      assertThat(count).isLessThanOrEqualTo(DIMENSIONS);
    });
  }

  @Test
  void indexEntryCount() {
    database.transaction(() -> {
      database.command("sql", "CREATE DOCUMENT TYPE TestDoc");
      database.command("sql", "CREATE PROPERTY TestDoc.vec ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX ON TestDoc (vec) LSM_VECTOR
          METADATA {"dimensions": 2,
          "similarity": "DOT_PRODUCT",
          "maxConnections": 4,
          "beamWidth": 10}
          """);
    });

    database.transaction(() -> {
      for (int i = 0; i < 50; i++) {
        final var doc = database.newDocument("TestDoc");
        doc.set("vec", new float[] { (float) i, (float) i * 2 });
        doc.save();
      }
    });

    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("TestDoc[vec]");
    assertThat(typeIndex.countEntries()).isEqualTo(50);
  }

  @Test
  void transactionalIsolation() {
    database.transaction(() -> {
      database.command("sql", "CREATE DOCUMENT TYPE TxDoc");
      database.command("sql", "CREATE PROPERTY TxDoc.vec ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX ON TxDoc (vec) LSM_VECTOR
          METADATA {
            "dimensions": 2,
            "similarity": "COSINE",
            "maxConnections": 4,
            "beamWidth": 10
          }""");
    });

    // Insert in transaction 1
    database.transaction(() -> {
      final var doc = database.newDocument("TxDoc");
      doc.set("vec", new float[] { 1.0f, 2.0f });
      doc.save();
    });

    final TypeIndex typeIndex = (TypeIndex) database.getSchema().getIndexByName("TxDoc[vec]");
    assertThat(typeIndex.countEntries()).isEqualTo(1);

    // Verify transaction rollback doesn't affect committed data
    try {
      database.transaction(() -> {
        final var doc = database.newDocument("TxDoc");
        doc.set("vec", new float[] { 3.0f, 4.0f });
        doc.save();

        // Force rollback
        throw new RuntimeException("Test rollback");
      });
    } catch (final RuntimeException e) {
      // Expected
    }

    // Should still have only 1 entry
    assertThat(typeIndex.countEntries()).isEqualTo(1);
  }

  @Test
  void lsmAppendOnlySemantics() {
    // Test LSM append-only: add, remove, add again - last entry should win
    database.transaction(() -> {
      database.command("sql", "CREATE DOCUMENT TYPE LSMDoc");
      database.command("sql", "CREATE PROPERTY LSMDoc.id STRING");
      database.command("sql", "CREATE PROPERTY LSMDoc.vec ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX ON LSMDoc (vec) LSM_VECTOR
          METADATA {
            "dimensions": 3,
            "similarity":
            "EUCLIDEAN",
            "maxConnections": 4,
            "beamWidth": 10
          }""");
    });

    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("LSMDoc[vec]");
    assertThat(typeIndex).isNotNull();

    // Add first version
    final RID[] docRidHolder = new RID[1];
    database.transaction(() -> {
      final var doc = database.newDocument("LSMDoc");
      doc.set("id", "doc1");
      doc.set("vec", new float[] { 1.0f, 2.0f, 3.0f });
      doc.save();
      docRidHolder[0] = doc.getIdentity();
    });
    final RID docRid = docRidHolder[0];

    assertThat(typeIndex.countEntries()).isEqualTo(1);

    // Query to verify it's there
    database.transaction(() -> {
      final float[] queryVec = { 1.0f, 2.0f, 3.0f };
      final var cursor = typeIndex.get(new Object[] { queryVec }, 10);
      int count = 0;
      while (cursor.hasNext()) {
        cursor.next();
        count++;
      }
      assertThat(count > 0).as("Should find the added vector").isTrue();
    });

    // Update the document (delete old entry, add new entry)
    database.transaction(() -> {
      final var doc = docRid.asDocument().modify();
      doc.set("vec", new float[] { 10.0f, 20.0f, 30.0f });
      doc.save();
    });

    // Should still have 1 entry (old marked deleted, new added)
    assertThat(typeIndex.countEntries()).isEqualTo(1);

    // Query with new vector should find it
    database.transaction(() -> {
      final float[] queryVec = { 10.0f, 20.0f, 30.0f };
      final var cursor = typeIndex.get(new Object[] { queryVec }, 10);
      int count = 0;
      while (cursor.hasNext()) {
        cursor.next();
        count++;
      }
      assertThat(count > 0).as("Should find the updated vector").isTrue();
    });

    // Note: RID stays the same after update, but vector value changed
    // With approximate search, we can't reliably test that old vector values aren't found
    // So we just verify the index still has 1 entry (the updated one)

    // Delete the document
    database.transaction(() -> {
      final var doc = docRid.asDocument();
      doc.delete();
    });

    // Should have 0 entries now
    assertThat(typeIndex.countEntries()).isEqualTo(0);

    // Re-add with same RID (simulating LSM append-only where entries accumulate)
    database.transaction(() -> {
      final var doc = database.newDocument("LSMDoc");
      doc.set("id", "doc1_new");
      doc.set("vec", new float[] { 100.0f, 200.0f, 300.0f });
      doc.save();
    });

    // Should have 1 entry (the new one)
    assertThat(typeIndex.countEntries()).isEqualTo(1);
  }

  @Test
  void lsmMergeOnReadLastWins() {
    // Test that in LSM pages, the last entry for a vector ID wins during merge-on-read
    database.transaction(() -> {
      database.command("sql", "CREATE DOCUMENT TYPE MergeDoc");
      database.command("sql", "CREATE PROPERTY MergeDoc.vec ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX ON MergeDoc (vec) LSM_VECTOR
          METADATA {
            "dimensions": 2,
            "similarity":
            "DOT_PRODUCT",
            "maxConnections": 4,
            "beamWidth": 10
          }""");
    });

    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("MergeDoc[vec]");

    // Add multiple documents
    final List<RID> rids = new ArrayList<>();
    database.transaction(() -> {
      for (int i = 0; i < 10; i++) {
        final var doc = database.newDocument("MergeDoc");
        doc.set("vec", new float[] { (float) i, (float) i * 2 });
        doc.save();
        rids.add(doc.getIdentity());
      }
    });

    assertThat(typeIndex.countEntries()).isEqualTo(10);

    // Update several documents multiple times (creates multiple page entries in LSM style)
    for (int iteration = 0; iteration < 3; iteration++) {
      final int iter = iteration;
      database.transaction(() -> {
        for (int i = 0; i < 5; i++) {
          final var doc = rids.get(i).asDocument().modify();
          doc.set("vec", new float[] { (float) (i + 100 * (iter + 1)), (float) (i * 2 + 100 * (iter + 1)) });
          doc.save();
        }
      });
    }

    // Still should have 10 entries (5 updated + 5 unchanged)
    // LSM semantics: last write wins, so even though we wrote multiple times,
    // only the latest version of each vector should be counted
    assertThat(typeIndex.countEntries()).isEqualTo(10);

    // Verify the last values are present (should find vectors with latest iteration values)
    database.transaction(() -> {
      final float[] queryVec = { 300.0f, 300.0f }; // Last iteration values for i=0
      final var cursor = typeIndex.get(new Object[] { queryVec }, 10);
      int count = 0;
      while (cursor.hasNext()) {
        cursor.next();
        count++;
      }
      assertThat(count > 0).as("Should find the last updated vector").isTrue();
    });

    // Verify LSM semantics: The index should correctly handle multiple updates to same documents
    // Key point: Even after 3 updates to 5 documents, we still have exactly 10 entries (not 10 + 5*3)
    // This demonstrates that LSM merge-on-read is working (last entry wins)
  }

  @Test
  void lazyGraphIndexRebuild() {
    // Test that graph index is only rebuilt when needed (lazy)
    database.transaction(() -> {
      database.command("sql", "CREATE DOCUMENT TYPE LazyDoc");
      database.command("sql", "CREATE PROPERTY LazyDoc.vec ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX ON LazyDoc (vec) LSM_VECTOR
          METADATA {
            "dimensions": 4,
            "similarity": "COSINE",
            "maxConnections": 8,
            "beamWidth": 50
          }""");
    });

    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("LazyDoc[vec]");

    // Add documents
    database.transaction(() -> {
      for (int i = 0; i < 20; i++) {
        final var doc = database.newDocument("LazyDoc");
        doc.set("vec", new float[] { (float) i, (float) i + 1, (float) i + 2, (float) i + 3 });
        doc.save();
      }
    });

    // Graph index should be marked dirty after commit (not rebuilt yet)
    // First query should trigger rebuild
    final long startTime = System.currentTimeMillis();
    database.transaction(() -> {
      final float[] queryVec = { 5.0f, 6.0f, 7.0f, 8.0f };
      final var cursor = typeIndex.get(new Object[] { queryVec }, 5);
      int count = 0;
      while (cursor.hasNext()) {
        cursor.next();
        count++;
      }
      assertThat(count > 0).as("Should find vectors after lazy rebuild").isTrue();
    });
    final long rebuildTime = System.currentTimeMillis() - startTime;

    // Second query should be faster (no rebuild needed)
    final long startTime2 = System.currentTimeMillis();
    database.transaction(() -> {
      final float[] queryVec = { 10.0f, 11.0f, 12.0f, 13.0f };
      final var cursor = typeIndex.get(new Object[] { queryVec }, 5);
      int count = 0;
      while (cursor.hasNext()) {
        cursor.next();
        count++;
      }
      assertThat(count > 0).as("Should find vectors without rebuild").isTrue();
    });
    final long noRebuildTime = System.currentTimeMillis() - startTime2;

    // Second query should be significantly faster (no rebuild overhead)
    // Note: This is a heuristic test, may be flaky in CI
    //System.out.println("Rebuild time: " + rebuildTime + "ms, No rebuild time: " + noRebuildTime + "ms");
  }

  @Test
  void compactionTrigger() {
    // Test that compaction is triggered after reaching the threshold
    database.transaction(() -> {
      database.command("sql", "CREATE VERTEX TYPE CompactDoc IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY CompactDoc.vec IF NOT EXISTS ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON CompactDoc (vec) LSM_VECTOR
          METADATA {
            dimensions: 4,
            similarity: 'COSINE'
          }""");
    });

    final var typeIndex = (TypeIndex) database.getSchema().getIndexByName("CompactDoc[vec]");
    assertThat(typeIndex).as("Index should exist").isNotNull();

    final var indexes = typeIndex.getIndexesOnBuckets();
    assertThat(indexes.length > 0).as("Should have at least one bucket index").isTrue();

    final LSMVectorIndex lsmIndex = (LSMVectorIndex) indexes[0];
    final int initialPages = lsmIndex.getCurrentMutablePages();

    // Get compaction threshold
    final int threshold = database.getConfiguration()
        .getValueAsInteger(GlobalConfiguration.INDEX_COMPACTION_MIN_PAGES_SCHEDULE);

    // Add enough vectors to trigger multiple page creations
    // Each page is 256KB. With variable encoding, vector metadata is ~4-10 bytes per entry
    // (vectorId + bucketId + position + deleted flag), so we can fit ~25K-60K entries per page.
    // We need to fill pages to trigger compaction.
    // Note: With variable encoding, we need MORE vectors to fill the same number of pages
    final int vectorsPerBatch = 50000; // Increased for variable encoding efficiency
    final int batches = Math.max(threshold + 2, 12); // Ensure we exceed threshold

    for (int batch = 0; batch < batches; batch++) {
      final int batchNum = batch;
      database.transaction(() -> {
        for (int i = 0; i < vectorsPerBatch; i++) {
          final var vertex = database.newVertex("CompactDoc");
          vertex.set("vec", new float[] {
              (float) (batchNum * vectorsPerBatch + i),
              (float) (batchNum * vectorsPerBatch + i + 1),
              (float) (batchNum * vectorsPerBatch + i + 2),
              (float) (batchNum * vectorsPerBatch + i + 3)
          });
          vertex.save();
        }
      });

      // Check if pages increased
//      if (lsmIndex.getCurrentMutablePages() > initialPages) {
//        System.out.println("After batch " + batch + ": mutablePages=" + lsmIndex.getCurrentMutablePages() +
//            ", threshold=" + threshold);
//      }
    }

    final int finalPages = lsmIndex.getCurrentMutablePages();
//    System.out.println("Initial pages: " + initialPages + ", Final pages: " + finalPages +
//        ", Threshold: " + threshold);

    // Verify that compaction would have been triggered
    assertThat(finalPages >= threshold || initialPages >= threshold).as("Should have reached compaction threshold at some point")
        .isTrue();
  }

  @Test
  void compactionMergeMultipleUpdates() {
    // Test K-way merge with multiple updates to same vectors
    database.transaction(() -> {
      database.command("sql", "CREATE VERTEX TYPE MergeDoc IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY MergeDoc.id IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY MergeDoc.vec IF NOT EXISTS ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON MergeDoc (vec) LSM_VECTOR
          METADATA {
            dimensions: 3,
            similarity: 'COSINE'
          }
          """);
    });

    final var typeIndex = (TypeIndex) database.getSchema().getIndexByName("MergeDoc[vec]");
    assertThat(typeIndex).as("Index should exist").isNotNull();

    // Add initial vectors
    final List<RID> rids = new ArrayList<>();
    database.transaction(() -> {
      for (int i = 0; i < 20; i++) {
        final var vertex = database.newVertex("MergeDoc");
        vertex.set("id", "doc" + i);
        vertex.set("vec", new float[] { (float) i, (float) i * 2, (float) i * 3 });
        vertex.save();
        rids.add(vertex.getIdentity());
      }
    });

    final long countAfterInsert = typeIndex.countEntries();
    assertThat(countAfterInsert).as("Should have 20 entries after insert").isEqualTo(20);

    // Update same vectors multiple times (creates multiple LSM entries)
    for (int iteration = 0; iteration < 5; iteration++) {
      final int iter = iteration;
      database.transaction(() -> {
        for (int i = 0; i < 10; i++) {
          final var doc = rids.get(i).asDocument().modify();
          doc.set("vec", new float[] {
              (float) (i + 100 * (iter + 1)),
              (float) (i * 2 + 100 * (iter + 1)),
              (float) (i * 3 + 100 * (iter + 1))
          });
          doc.save();
        }
      });
    }

    // Count should still be 20 (no duplicates despite multiple updates)
    final long countAfterUpdates = typeIndex.countEntries();
    assertThat(countAfterUpdates).as("Should still have 20 entries after updates (LSM last-write-wins semantics)").isEqualTo(20);

    // Verify we can query the updated vectors
    database.transaction(() -> {
      final float[] queryVec = { 500.0f, 500.0f, 500.0f }; // Query for last iteration values
      final var cursor = typeIndex.get(new Object[] { queryVec }, 5);
      int count = 0;
      while (cursor.hasNext()) {
        cursor.next();
        count++;
      }
      assertThat(count > 0).as("Should find vectors with updated values").isTrue();
    });
  }

  @Test
  void compactionDeletedEntryRemoval() {
    // Test that deleted entries are removed during compaction
    database.transaction(() -> {
      database.command("sql", "CREATE VERTEX TYPE DeleteDoc IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY DeleteDoc.id IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY DeleteDoc.vec IF NOT EXISTS ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON DeleteDoc (vec) LSM_VECTOR
          METADATA {
            dimensions: 2,
            similarity: 'COSINE'
          }""");
    });

    final var typeIndex = (TypeIndex) database.getSchema().getIndexByName("DeleteDoc[vec]");
    assertThat(typeIndex).as("Index should exist").isNotNull();

    // Add vectors
    final List<RID> rids = new ArrayList<>();
    database.transaction(() -> {
      for (int i = 0; i < 50; i++) {
        final var vertex = database.newVertex("DeleteDoc");
        vertex.set("id", "doc" + i);
        vertex.set("vec", new float[] { (float) i, (float) i * 2 });
        vertex.save();
        rids.add(vertex.getIdentity());
      }
    });

    assertThat(typeIndex.countEntries()).as("Should have 50 entries").isEqualTo(50);

    // Delete half of the vectors
    database.transaction(() -> {
      for (int i = 0; i < 25; i++) {
        rids.get(i).asDocument().delete();
      }
    });

    // Count should reflect deletions
    final long countAfterDelete = typeIndex.countEntries();
    assertThat(countAfterDelete).as("Should have 25 entries after deleting 25").isEqualTo(25);

    // Verify remaining vectors are queryable
    database.transaction(() -> {
      final float[] queryVec = { 30.0f, 60.0f }; // Vector from non-deleted range
      final var cursor = typeIndex.get(new Object[] { queryVec }, 10);
      int count = 0;
      while (cursor.hasNext()) {
        cursor.next();
        count++;
      }
      assertThat(count > 0).as("Should find non-deleted vectors").isTrue();
    });

    // After compaction, deleted entries should be physically removed
    // (We can't directly test this without triggering manual compaction,
    // but the count verification above confirms logical deletion works)
  }

  @Test
  void compactionWithConcurrentQueries() throws Exception {
    // Test that queries work correctly during compaction
    database.transaction(() -> {
      database.command("sql", "CREATE VERTEX TYPE ConcurrentDoc IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY ConcurrentDoc.vec IF NOT EXISTS ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON ConcurrentDoc (vec) LSM_VECTOR
          METADATA {
            dimensions: 4,
            similarity: 'COSINE'
          }""");
    });

    final var typeIndex = (TypeIndex) database.getSchema().getIndexByName("ConcurrentDoc[vec]");
    assertThat(typeIndex).as("Index should exist").isNotNull();

    // Add initial vectors
    database.transaction(() -> {
      for (int i = 0; i < 100; i++) {
        final var vertex = database.newVertex("ConcurrentDoc");
        vertex.set("vec", new float[] {
            (float) i, (float) i + 1, (float) i + 2, (float) i + 3
        });
        vertex.save();
      }
    });

    assertThat(typeIndex.countEntries()).as("Should have 100 entries").isEqualTo(100);

    // Perform concurrent queries while index is active
    final int numThreads = 5;
    final int queriesPerThread = 10;
    final List<Thread> threads = new ArrayList<>();
    final AtomicInteger successfulQueries =
        new AtomicInteger(0);
    final AtomicInteger failedQueries =
        new AtomicInteger(0);

    for (int t = 0; t < numThreads; t++) {
      final int threadId = t;
      final Thread thread = new Thread(() -> {
        for (int q = 0; q < queriesPerThread; q++) {
          final int queryId = q; // Make it final for lambda
          try {
            database.transaction(() -> {
              final float[] queryVec = {
                  (float) (threadId * 10 + queryId),
                  (float) (threadId * 10 + queryId + 1),
                  (float) (threadId * 10 + queryId + 2),
                  (float) (threadId * 10 + queryId + 3)
              };
              final var cursor = typeIndex.get(new Object[] { queryVec }, 5);
              int count = 0;
              while (cursor.hasNext()) {
                cursor.next();
                count++;
              }
              // Should find at least some results
              if (count > 0) {
                successfulQueries.incrementAndGet();
              }
            });
          } catch (final Exception e) {
            failedQueries.incrementAndGet();
            System.err.println("Query failed: " + e.getMessage());
          }
        }
      });
      threads.add(thread);
      thread.start();
    }

    // Wait for all threads
    for (final Thread thread : threads) {
      thread.join();
    }

//    System.out.println("Successful queries: " + successfulQueries.get() +
//        ", Failed queries: " + failedQueries.get());

    // Most queries should succeed
    assertThat(successfulQueries.get() > (numThreads * queriesPerThread) / 2).as("Most concurrent queries should succeed").isTrue();
    assertThat(failedQueries.get()).as("No queries should fail with exceptions").isEqualTo(0);
  }

  @Test
  void graphIndexCorrectnessAfterCompaction() {
    // Test that graph index is correctly rebuilt after compaction
    database.transaction(() -> {
      database.command("sql", "CREATE VERTEX TYPE GraphDoc IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY GraphDoc.vec IF NOT EXISTS ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON GraphDoc (vec) LSM_VECTOR
          METADATA {
            dimensions: 8,
            similarity: 'EUCLIDEAN',
            maxConnections: 16,
            beamWidth: 100
          }""");
    });

    final var typeIndex = (TypeIndex) database.getSchema().getIndexByName("GraphDoc[vec]");
    assertThat(typeIndex).as("Index should exist").isNotNull();

    // Add vectors in a pattern that creates a structure in the graph
    final List<RID> rids = new ArrayList<>();
    database.transaction(() -> {
      for (int i = 0; i < 100; i++) {
        final var vertex = database.newVertex("GraphDoc");
        // Create vectors in clusters (should form neighborhoods in graph)
        final int cluster = i / 10;
        vertex.set("vec", new float[] {
            (float) cluster * 10, (float) cluster * 10 + 1,
            (float) cluster * 10 + 2, (float) cluster * 10 + 3,
            (float) i % 10, (float) (i % 10) + 1,
            (float) (i % 10) + 2, (float) (i % 10) + 3
        });
        vertex.save();
        rids.add(vertex.getIdentity());
      }
    });

    assertThat(typeIndex.countEntries()).as("Should have 100 entries").isEqualTo(100);

    // Query before any updates (graph index initialized)
    final Set<String> resultsBeforeUpdates = new HashSet<>();
    database.transaction(() -> {
      final float[] queryVec = { 50.0f, 51.0f, 52.0f, 53.0f, 0.0f, 1.0f, 2.0f, 3.0f };
      final var cursor = typeIndex.get(new Object[] { queryVec }, 10);
      while (cursor.hasNext()) {
        final var rid = cursor.next();
        resultsBeforeUpdates.add(rid.getIdentity().toString());
      }
    });

    assertThat(resultsBeforeUpdates.size() > 0).as("Should find results before updates").isTrue();

    // Update some vectors (will trigger graph dirty flag)
    database.transaction(() -> {
      for (int i = 0; i < 20; i++) {
        final var doc = rids.get(i).asDocument().modify();
        final int cluster = i / 10;
        doc.set("vec", new float[] {
            (float) cluster * 10, (float) cluster * 10 + 1,
            (float) cluster * 10 + 2, (float) cluster * 10 + 3,
            (float) i % 10 + 0.1f, (float) (i % 10) + 1.1f,
            (float) (i % 10) + 2.1f, (float) (i % 10) + 3.1f
        });
        doc.save();
      }
    });

    // Query after updates (should trigger lazy graph rebuild)
    final Set<String> resultsAfterUpdates = new HashSet<>();
    database.transaction(() -> {
      final float[] queryVec = { 50.0f, 51.0f, 52.0f, 53.0f, 0.1f, 1.1f, 2.1f, 3.1f };
      final var cursor = typeIndex.get(new Object[] { queryVec }, 10);
      while (cursor.hasNext()) {
        final var rid = cursor.next();
        resultsAfterUpdates.add(rid.getIdentity().toString());
      }
    });

    assertThat(resultsAfterUpdates.size() > 0).as("Should find results after updates (graph index rebuilt)").isTrue();

    // Results should be different after updates (graph index reflects new data)
//    System.out.println("Results before updates: " + resultsBeforeUpdates.size() +
//        ", Results after updates: " + resultsAfterUpdates.size());
  }

  @Test
  void transactionRollbackVectorIndexChanges() {
    // Setup: Create a vector index
    database.transaction(() -> {
      database.command("sql", "CREATE DOCUMENT TYPE RollbackDoc");
      database.command("sql", "CREATE PROPERTY RollbackDoc.id STRING");
      database.command("sql", "CREATE PROPERTY RollbackDoc.vec ARRAY_OF_FLOATS");
      database.command("sql", """
          CREATE INDEX ON RollbackDoc (vec) LSM_VECTOR
          METADATA {
            "dimensions": 3,
            "similarity": "COSINE",
            "maxConnections": 8,
            "beamWidth": 50
          }""");
    });

    final TypeIndex typeIndex =
        (TypeIndex) database.getSchema().getIndexByName("RollbackDoc[vec]");
    assertThat(typeIndex).as("Vector index should be created").isNotNull();

    // Verify index is initially empty
    assertThat(typeIndex.countEntries()).as("Index should start empty").isEqualTo(0);

    // Add initial documents to the index
    database.transaction(() -> {
      for (int i = 0; i < 5; i++) {
        final var doc = database.newDocument("RollbackDoc");
        doc.set("id", "initial_" + i);
        doc.set("vec", new float[] { (float) i, (float) i + 1, (float) i + 2 });
        doc.save();
      }
    });

    final long initialCount = typeIndex.countEntries();
    assertThat(initialCount).as("Index should have 5 entries after initial insert").isEqualTo(5);

    // === ROLLBACK TEST ===
    // Try to add more documents in a transaction, then roll it back
    try {
      database.transaction(() -> {
        // Add new documents in this transaction
        for (int i = 0; i < 3; i++) {
          final var doc = database.newDocument("RollbackDoc");
          doc.set("id", "rollback_" + i);
          doc.set("vec", new float[] { (float) (100 + i), (float) (101 + i), (float) (102 + i) });
          doc.save();
        }

        // Force a rollback by throwing an exception
        throw new RuntimeException("Intentional rollback for testing");
      });
    } catch (final RuntimeException e) {
      // Expected - transaction should be rolled back
      assertThat(e.getMessage().contains("Intentional rollback")).as("Should be our rollback exception").isTrue();
    }

    // Verify that the rolled-back changes are NOT in the index
    final long countAfterRollback = typeIndex.countEntries();
    assertThat(countAfterRollback).as(
            "Index should still have only " + initialCount + " entries after rollback (rollback_* docs should not be there)")
        .isEqualTo(initialCount);

    // Verify that we can't find the rolled-back vectors
    database.transaction(() -> {
      final float[] rollbackVec = { 100.0f, 101.0f, 102.0f }; // Vector that should have been rolled back
      final IndexCursor cursor = typeIndex.get(new Object[] { rollbackVec }, 10);

      int foundCount = 0;
      while (cursor.hasNext()) {
        final var rid = cursor.next();
        final var doc = rid.asDocument();
        // We shouldn't find any documents with rollback_* ids
        final String id = doc.get("id").toString();
        assertThat(id.startsWith("rollback_")).as("Should not find rolled-back vector in index (found doc with id: " + id + ")")
            .isFalse();
        foundCount++;
      }

      // With approximate nearest neighbor search, we might find some initial vectors,
      // but we should definitely NOT find the exact rolled-back vectors
      // Just verify we didn't find the specific rollback documents
    });

    // === COMMIT TEST ===
    // Now add documents in a transaction that COMMITS successfully
    database.transaction(() -> {
      for (int i = 0; i < 2; i++) {
        final var doc = database.newDocument("RollbackDoc");
        doc.set("id", "committed_" + i);
        doc.set("vec", new float[] { (float) (200 + i), (float) (201 + i), (float) (202 + i) });
        doc.save();
      }
    });

    // Verify that the committed changes ARE in the index
    final long countAfterCommit = typeIndex.countEntries();
    assertThat(countAfterCommit).as("Index should have " + (initialCount + 2) + " entries after successful commit")
        .isEqualTo(initialCount + 2);

    // Verify that we CAN find the committed vectors
    database.transaction(() -> {
      final float[] committedVec = { 200.0f, 201.0f, 202.0f }; // Vector that was committed
      final IndexCursor cursor = typeIndex.get(new Object[] { committedVec }, 10);

      boolean found = false;
      while (cursor.hasNext()) {
        final var rid = cursor.next();
        final var doc = rid.asDocument();
        final String id = doc.get("id").toString();
        if (id.equals("committed_0")) {
          found = true;
          break;
        }
      }

      assertThat(found).as("Should find the committed vector in index").isTrue();
    });

    // === MULTIPLE ROLLBACK TEST ===
    // Test multiple consecutive rollbacks
    final long countBeforeMultipleRollbacks = typeIndex.countEntries();

    for (int rollbackRound = 0; rollbackRound < 3; rollbackRound++) {
      final int round = rollbackRound; // Capture for lambda
      try {
        database.transaction(() -> {
          for (int i = 0; i < 2; i++) {
            final var doc = database.newDocument("RollbackDoc");
            doc.set("id", "rollback_round_" + round + "_" + i);
            doc.set("vec", new float[] { (float) (300 + round), (float) (301 + round), (float) (302 + round) });
            doc.save();
          }
          throw new RuntimeException("Rollback round " + round);
        });
      } catch (final RuntimeException e) {
        // Expected
      }
    }

    // After multiple rollbacks, count should not have changed
    final long countAfterMultipleRollbacks = typeIndex.countEntries();
    assertThat(countAfterMultipleRollbacks)
        .as("Index count should remain unchanged after multiple rollbacks")
        .isEqualTo(countBeforeMultipleRollbacks);

    // Verify no rolled-back documents from any round are in the index
    database.transaction(() -> {
      for (int rollbackRound = 0; rollbackRound < 3; rollbackRound++) {
        final float[] rollbackVec = { (float) (300 + rollbackRound), (float) (301 + rollbackRound), (float) (302 + rollbackRound) };
        final IndexCursor cursor = typeIndex.get(new Object[] { rollbackVec }, 100);

        while (cursor.hasNext()) {
          final var rid = cursor.next();
          final var doc = rid.asDocument();
          final String id = doc.get("id").toString();
          assertThat(id.startsWith("rollback_round_"))
              .as("Should not find rolled-back vectors from any round (found: " + id + ")")
              .isFalse();
        }
      }
    });
  }

  @Test
  void manualCompactionRemovesDuplicates() throws Exception {
    // Test that manual compaction correctly merges pages and removes duplicates
    database.transaction(() -> {
      database.command("sql", "CREATE VERTEX TYPE CompactTest IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY CompactTest.id IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY CompactTest.vec IF NOT EXISTS ARRAY_OF_FLOATS");
      database.command("sql", "CREATE INDEX IF NOT EXISTS ON CompactTest (vec) LSM_VECTOR " +
          "METADATA {dimensions: 4, similarity: 'COSINE'}");
    });

    final var typeIndex = (TypeIndex) database.getSchema().getIndexByName("CompactTest[vec]");
    assertThat(typeIndex).as("Index should exist").isNotNull();

    final var indexes = typeIndex.getIndexesOnBuckets();
    assertThat(indexes.length > 0).as("Should have at least one bucket index").isTrue();
    final LSMVectorIndex lsmIndex = (LSMVectorIndex) indexes[0];

    // Track initial state
    final int initialPages = lsmIndex.getTotalPages();
    final int initialMutablePages = lsmIndex.getCurrentMutablePages();

    // Add initial vectors - need many to fill pages (pages are 256KB)
    // With 4-dim floats: 4 * 4 bytes = 16 bytes per vector + overhead (~60 bytes total per entry)
    // Need ~4,000 vectors to fill a page, so we need significantly more to ensure at least one full page
    final int numVectors = 30000; // Enough to create multiple full pages
    final List<RID> rids = new ArrayList<>();
    database.transaction(() -> {
      for (int i = 0; i < numVectors; i++) {
        final var vertex = database.newVertex("CompactTest");
        vertex.set("id", "doc" + i);
        vertex.set("vec", new float[] { (float) i, (float) i + 1, (float) i + 2, (float) i + 3 });
        vertex.save();
        rids.add(vertex.getIdentity());
      }
    });

    final long countAfterInsert = typeIndex.countEntries();
    assertThat(countAfterInsert).as("Should have " + numVectors + " entries after insert").isEqualTo(numVectors);

    // Update the same vectors multiple times to create duplicates in LSM pages
    final int numUpdates = 5000; // Update first 5000 vectors
    for (int iteration = 0; iteration < 5; iteration++) {
      final int iter = iteration;
      database.transaction(() -> {
        for (int i = 0; i < numUpdates; i++) {
          final var doc = rids.get(i).asDocument().modify();
          doc.set("vec", new float[] {
              (float) (i + 100 * (iter + 1)),
              (float) (i + 100 * (iter + 1) + 1),
              (float) (i + 100 * (iter + 1) + 2),
              (float) (i + 100 * (iter + 1) + 3)
          });
          doc.save();
        }
      });
    }

    // After multiple updates, LSM index should have many entries in pages (duplicates)
    // but countEntries should still report numVectors (last-write-wins semantics)
    final long countAfterUpdates = typeIndex.countEntries();
    assertThat(countAfterUpdates).as("Should still have " + numVectors + " entries (LSM merge-on-read)").isEqualTo(numVectors);

    final int pagesBeforeCompaction = lsmIndex.getTotalPages();
//    System.out.println("Pages before compaction: " + pagesBeforeCompaction +
//        ", mutable: " + lsmIndex.getCurrentMutablePages());

    // Check which pages are actually immutable by reading the mutable flag directly
    database.transaction(() -> {
      final DatabaseInternal dbInternal = (DatabaseInternal) database;
      int immutableCount = 0;
      for (int pageNum = 1; pageNum < pagesBeforeCompaction; pageNum++) {
        try {
          final var page = dbInternal.getTransaction().getPage(
              new PageId(dbInternal, lsmIndex.getFileId(), pageNum),
              lsmIndex.getPageSize());
          final ByteBuffer buffer = page.getContent();
          buffer.position(8); // OFFSET_MUTABLE = 8
          final byte mutable = buffer.get();
          if (mutable == 0) {
            immutableCount++;
          }
//          System.out.println("  Page " + pageNum + ": mutable=" + mutable);
        } catch (Exception e) {
          System.out.println("  Page " + pageNum + ": error reading - " + e.getMessage());
        }
      }
//      System.out.println("Found " + immutableCount + " immutable pages");
    });

    // Manually trigger compaction
//    System.out.println("Attempting to schedule compaction...");

    // Check pre-conditions
    final DatabaseInternal dbInternal2 = (DatabaseInternal) database;
//    System.out.println("Database open: " + dbInternal2.isOpen());
//    System.out.println("Database mode: " + dbInternal2.getMode());
//    System.out.println("Page flushing suspended: " + dbInternal2.getPageManager().isPageFlushingSuspended(dbInternal2));
//    System.out.println("Transaction active: " + dbInternal2.isTransactionActive());

    boolean compactionScheduled = lsmIndex.scheduleCompaction();
//    System.out.println("Compaction scheduled: " + compactionScheduled);

    // Try a small delay in case there's async processing
    Thread.sleep(100);

//    System.out.println("Calling compact()...");
    boolean compacted = lsmIndex.compact();
//    System.out.println("Compaction result: " + compacted);

    // If first attempt failed, try scheduling and compacting again
    if (!compacted) {
//      System.out.println("First compaction attempt failed, trying again...");
      Thread.sleep(100);
      compactionScheduled = lsmIndex.scheduleCompaction();
//      System.out.println("Second compaction scheduled: " + compactionScheduled);
      compacted = lsmIndex.compact();
//      System.out.println("Second compaction result: " + compacted);
    }

    if (compacted) {
//      System.out.println("Compaction succeeded!");

      // After compaction, verify the index still works correctly
      final long countAfterCompaction = typeIndex.countEntries();
      assertThat(countAfterCompaction).as("Should still have " + numVectors + " entries after compaction").isEqualTo(numVectors);

      // Verify we can still query the index
      database.transaction(() -> {
        final float[] queryVec = { 500.0f, 501.0f, 502.0f, 503.0f }; // Last iteration values for first doc
        final var cursor = typeIndex.get(new Object[] { queryVec }, 10);
        int count = 0;
        while (cursor.hasNext()) {
          cursor.next();
          count++;
        }
        assertThat(count > 0).as("Should find vectors after compaction").isTrue();
      });

      // Check if compacted sub-index was created
//      final var compactedSubIndex = lsmIndex.getSubIndex();
//      if (compactedSubIndex != null) {
//        System.out.println("Compacted sub-index created: fileId=" + compactedSubIndex.getFileId() +
//            ", pages=" + compactedSubIndex.getTotalPages());
//      }
    } else {
      System.out.println("Compaction failed or returned no changes. This could mean:");
      System.out.println("  - Status was not COMPACTION_SCHEDULED when compact() was called");
      System.out.println("  - Database is closed or page flushing is suspended");
      System.out.println("  - No immutable pages found");
      System.out.println("  - No entries were compacted (all deleted or empty pages)");
    }

    // Verify sample documents still exist with correct latest values
    database.transaction(() -> {
      // Check first 100 and last 100 documents as a sample
      for (int i = 0; i < Math.min(100, numVectors); i++) {
        final var doc = rids.get(i).asDocument();
        assertThat(doc).as("Document " + i + " should still exist").isNotNull();

        final float[] vec = (float[]) doc.get("vec");
        assertThat(vec).as("Vector should exist for document " + i).isNotNull();
        assertThat(vec.length).as("Vector should have 4 dimensions").isEqualTo(4);

        // First numUpdates docs were updated 5 times, rest kept original values
        if (i < numUpdates) {
          // Should have last iteration values (iter=4)
          final float expectedFirst = i + 100 * 5;
          assertThat(vec[0]).as("Vector[0] for doc " + i + " should have latest value").isEqualTo(expectedFirst);
        } else {
          // Should have original values
          assertThat(vec[0]).as("Vector[0] for doc " + i + " should have original value").isEqualTo((float) i);
        }
      }

      // Check last 100 documents
      for (int i = Math.max(0, numVectors - 100); i < numVectors; i++) {
        final var doc = rids.get(i).asDocument();
        assertThat(doc).as("Document " + i + " should still exist").isNotNull();
        final float[] vec = (float[]) doc.get("vec");
        assertThat(vec).as("Vector should exist for document " + i).isNotNull();
        // These should have original values (not updated)
        assertThat(vec[0]).as("Vector[0] for doc " + i + " should have original value").isEqualTo((float) i);
      }
    });
  }

  @Test
  void testVectorNeighborsViaSQL() {
    database.transaction(() -> {
      // Create vertex type with vector property
      database.command("sql", "CREATE VERTEX TYPE Product IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY Product.name IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY Product.embedding IF NOT EXISTS ARRAY_OF_FLOATS");

      // Create LSM vector index on embedding property
      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON Product (embedding) LSM_VECTOR
          METADATA {
            "dimensions": 128,
            "similarity": "COSINE",
            "maxConnections": 16,
            "beamWidth": 100
          }""");
    });

    // Insert test data with 128-dimensional vectors
    final int numDocs = 50;
    final List<String> productNames = new ArrayList<>();

    database.transaction(() -> {
      for (int i = 0; i < numDocs; i++) {
        final float[] embedding = new float[128];
        // Create vectors with patterns:
        // - First 10 vectors cluster around [1.0, 0.0, 0.0, ...]
        // - Next 10 vectors cluster around [0.0, 1.0, 0.0, ...]
        // - Next 10 vectors cluster around [0.0, 0.0, 1.0, ...]
        // - Remaining vectors are more random
        if (i < 10) {
          embedding[0] = 1.0f + (i * 0.1f);
          embedding[1] = 0.1f * i;
        } else if (i < 20) {
          embedding[0] = 0.1f * (i - 10);
          embedding[1] = 1.0f + ((i - 10) * 0.1f);
        } else if (i < 30) {
          embedding[0] = 0.1f * (i - 20);
          embedding[1] = 0.1f * (i - 20);
          embedding[2] = 1.0f + ((i - 20) * 0.1f);
        } else {
          // Random-ish vectors for the rest
          for (int j = 0; j < 128; j++) {
            embedding[j] = (float) Math.sin(i * j * 0.01);
          }
        }

        final String name = "Product_" + i;
        productNames.add(name);

        database.command("sql",
            "INSERT INTO Product SET name = ?, embedding = ?",
            name, embedding);
      }
    });

//    System.out.println("Inserted " + numDocs + " products with 128-dimensional vectors");

    // Test 1: Find neighbors of first product (should find products 1-9 as nearest neighbors)
    database.transaction(() -> {
      final var result = database.query("sql",
          "SELECT name, vectorNeighbors('Product[embedding]', embedding, 5) as neighbors FROM Product WHERE name = 'Product_0'");

      assertThat(result.hasNext()).as("Query should return results").isTrue();
      final var doc = result.next();
      final String name = doc.getProperty("name");
      assertThat(name).as("Should get Product_0").isEqualTo("Product_0");

      // The neighbors should include other products from cluster 0-9
//      System.out.println("Neighbors of Product_0: " + doc.toJSON());
    });

    // Test 2: Query using vectorNeighbors with arbitrary query vector
    database.transaction(() -> {
      // Create a query vector similar to cluster 1 (second cluster)
      final float[] queryVector = new float[128];
      queryVector[1] = 1.0f; // Similar to products 10-19

      // Use vectorNeighbors to find nearest neighbors
      final var result = database.query("sql",
          "SELECT name, vectorNeighbors('Product[embedding]', ?, 5) as neighbors FROM Product LIMIT 1",
          queryVector);

      assertThat(result.hasNext()).as("Query should return results").isTrue();
//      System.out.println("VectorNeighbors result for cluster 1 query: " + result.next().toJSON());
    });

    // Test 3: Test vectorNeighbors function with different k value
    database.transaction(() -> {
      final float[] queryVector = new float[128];
      queryVector[2] = 1.0f; // Similar to products 20-29

      final var result = database.query("sql",
          "SELECT name, vectorNeighbors('Product[embedding]', ?, 10) as neighbors FROM Product LIMIT 1",
          queryVector);

      assertThat(result.hasNext()).as("Query should return results").isTrue();

      final var doc = result.next();
//      System.out.println("VectorNeighbors result for cluster 2 query (k=10): " + doc.toJSON());
    });

    // Test 4: Query with specific product and find its nearest neighbors
    database.transaction(() -> {
      final var result = database.query("sql",
          "SELECT name, vectorNeighbors('Product[embedding]', embedding, 3) as neighbors " +
              "FROM Product WHERE name = 'Product_15'");

      assertThat(result.hasNext()).as("Query should return results").isTrue();
      final var doc = result.next();
//      System.out.println("Neighbors of Product_15: " + doc.toJSON());

      // Product_15 should be similar to other products in the 10-19 range
      final String productName = doc.getProperty("name");
      assertThat(productName).isEqualTo("Product_15");
    });

    // Test 5: Verify multiple queries work correctly
    database.transaction(() -> {
      final float[] queryVector1 = new float[128];
      queryVector1[0] = 1.0f;

      final var result1 = database.query("sql",
          "SELECT name, vectorNeighbors('Product[embedding]', ?, 3) as neighbors FROM Product LIMIT 1",
          queryVector1);

      assertThat(result1.hasNext()).as("First query should return results").isTrue();
//      System.out.println("Query 1 result: " + result1.next().toJSON());

      final float[] queryVector2 = new float[128];
      queryVector2[1] = 1.0f;

      final var result2 = database.query("sql",
          "SELECT name, vectorNeighbors('Product[embedding]', ?, 3) as neighbors FROM Product LIMIT 1",
          queryVector2);

      assertThat(result2.hasNext()).as("Second query should return results").isTrue();
//      System.out.println("Query 2 result: " + result2.next().toJSON());
    });

//    System.out.println("â All SQL vectorNeighbors tests passed!");
  }

  @Test
  void testVariableEncodingRoundTrip() {
    database.transaction(() -> {
      // Create vertex type and index
      database.command("sql", "CREATE VERTEX TYPE VectorDoc IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY VectorDoc.name IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY VectorDoc.embedding IF NOT EXISTS ARRAY_OF_FLOATS");

      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON VectorDoc (embedding) LSM_VECTOR
          METADATA {
            "dimensions": 128,
            "similarity": "COSINE",
            "maxConnections": 16,
            "beamWidth": 100
          }""");
    });

    // Test small IDs (should use 1 byte with variable encoding)
    database.transaction(() -> {
      for (int i = 0; i < 10; i++) {
        final float[] embedding = new float[128];
        embedding[0] = (float) i;
        database.command("sql", "INSERT INTO VectorDoc SET name = ?, embedding = ?", "small_" + i, embedding);
      }
    });

    // Test medium IDs (should use 2-3 bytes with variable encoding)
    database.transaction(() -> {
      for (int i = 1000; i < 1010; i++) {
        final float[] embedding = new float[128];
        embedding[0] = (float) i;
        database.command("sql", "INSERT INTO VectorDoc SET name = ?, embedding = ?", "medium_" + i, embedding);
      }
    });

    // Test large IDs (should use 4-5 bytes with variable encoding)
    database.transaction(() -> {
      for (int i = 100000; i < 100010; i++) {
        final float[] embedding = new float[128];
        embedding[0] = (float) i;
        database.command("sql", "INSERT INTO VectorDoc SET name = ?, embedding = ?", "large_" + i, embedding);
      }
    });

    // Verify all entries can be read back correctly
    database.transaction(() -> {
      final var result = database.query("sql", "SELECT COUNT(*) as count FROM VectorDoc");
      assertThat(result.hasNext()).isTrue();
      final long count = result.next().getProperty("count");
      assertThat(count).as("Should have inserted 30 documents").isEqualTo(30L);
    });

    // Verify vector search still works correctly with variable-sized entries
    database.transaction(() -> {
      final float[] queryVector = new float[128];
      queryVector[0] = 5.0f; // Should be closest to small IDs 4-6

      final var result = database.query("sql",
          "SELECT name, vectorNeighbors('VectorDoc[embedding]', ?, 3) as neighbors FROM VectorDoc LIMIT 1",
          queryVector);

      assertThat(result.hasNext()).as("Query should return results").isTrue();
//      System.out.println("Variable encoding round-trip test: " + result.next().toJSON());
    });

//    System.out.println("â Variable encoding round-trip test passed!");
  }

  @Test
  void testAbsoluteFileOffsets() {
    database.transaction(() -> {
      // Create vertex type and index
      database.command("sql", "CREATE VERTEX TYPE OffsetTest IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY OffsetTest.name IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY OffsetTest.embedding IF NOT EXISTS ARRAY_OF_FLOATS");

      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON OffsetTest (embedding) LSM_VECTOR
          METADATA {
            "dimensions": 64,
            "similarity": "COSINE",
            "maxConnections": 16,
            "beamWidth": 100
          }""");
    });

    // Insert entries with varying sizes
    final List<String> insertedNames = new ArrayList<>();
    database.transaction(() -> {
      for (int i = 0; i < 50; i++) {
        final float[] embedding = new float[64];
        for (int j = 0; j < 64; j++) {
          embedding[j] = (float) (Math.sin(i + j) * 0.5);
        }
        final String name = "doc_" + i;
        insertedNames.add(name);
        database.command("sql", "INSERT INTO OffsetTest SET name = ?, embedding = ?", name, embedding);
      }
    });

    // Get the index and verify VectorLocationIndex has absolute file offsets
    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("OffsetTest[embedding]");
    assertThat(typeIndex).as("Index should exist").isNotNull();

    final LSMVectorIndex lsmIndex = (LSMVectorIndex) typeIndex.getIndexesOnBuckets()[0];
    final VectorLocationIndex vectorIndex = lsmIndex.getVectorIndex();

    // Verify we have entries in the index
    assertThat(vectorIndex.size()).as("Should have 50 entries").isEqualTo(50);

    // Verify all entries have valid absolute file offsets
    final int[] validOffsetCount = { 0 };
    vectorIndex.getAllVectorIds().forEach(vectorId -> {
      final VectorLocationIndex.VectorLocation location = vectorIndex.getLocation(vectorId);
      assertThat(location).as("Location should exist for vectorId " + vectorId).isNotNull();
      assertThat(location.absoluteFileOffset).as("Offset should be non-negative").isGreaterThanOrEqualTo(0);
      assertThat(location.rid).as("RID should be set").isNotNull();
      validOffsetCount[0]++;
    });

    assertThat(validOffsetCount[0]).as("Should have validated 50 offsets").isEqualTo(50);

    // Verify vector search works (implicitly tests that offsets are correct)
    database.transaction(() -> {
      final float[] queryVector = new float[64];
      Arrays.fill(queryVector, 0.5f);

      final var result = database.query("sql",
          "SELECT name, vectorNeighbors('OffsetTest[embedding]', ?, 5) as neighbors FROM OffsetTest LIMIT 1",
          queryVector);

      assertThat(result.hasNext()).as("Query should return results").isTrue();
//      System.out.println("Absolute offset test query: " + result.next().toJSON());
    });

//    System.out.println("â Absolute file offset test passed!");
  }

  @Test
  void testPageBoundaryHandling() {
    database.transaction(() -> {
      // Create vertex type and index with small page size to force page boundaries
      database.command("sql", "CREATE VERTEX TYPE BoundaryTest IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY BoundaryTest.name IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY BoundaryTest.embedding IF NOT EXISTS ARRAY_OF_FLOATS");

      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON BoundaryTest (embedding) LSM_VECTOR
          METADATA {
            "dimensions": 256,
            "similarity": "COSINE",
            "maxConnections": 16,
            "beamWidth": 100
          }""");
    });

    // Insert enough entries to span multiple pages
    // With 256-dimensional vectors and variable encoding, this should create multiple pages
    final int numEntries = 5000;
    database.transaction(() -> {
      for (int i = 0; i < numEntries; i++) {
        final float[] embedding = new float[256];
        for (int j = 0; j < 256; j++) {
          embedding[j] = (float) Math.random();
        }
        database.command("sql", "INSERT INTO BoundaryTest SET name = ?, embedding = ?", "boundary_" + i, embedding);
      }
    });

    // Verify all entries were inserted
    database.transaction(() -> {
      final var result = database.query("sql", "SELECT COUNT(*) as count FROM BoundaryTest");
      assertThat(result.hasNext()).isTrue();
      final long count = result.next().getProperty("count");
      assertThat(count).as("Should have inserted all entries").isEqualTo((long) numEntries);
    });

    // Get the index and check page structure
    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("BoundaryTest[embedding]");
    final LSMVectorIndex lsmIndex = (LSMVectorIndex) typeIndex.getIndexesOnBuckets()[0];

    final int totalPages = lsmIndex.getTotalPages();
    assertThat(totalPages).as("Should have created at least 1 page").isGreaterThanOrEqualTo(1);
//    System.out.println("Created " + totalPages + " pages for " + numEntries + " entries");

    // Note: With variable encoding, metadata is very compact (~4-10 bytes per entry)
    // So 5000 entries Ã 7 bytes â 35KB, which fits in a single 256KB page.
    // This test validates that page boundary handling works correctly when it occurs.

    // Verify vector search works across page boundaries
    database.transaction(() -> {
      final float[] queryVector = new float[256];
      Arrays.fill(queryVector, 0.5f);

      final var result = database.query("sql",
          "SELECT name, vectorNeighbors('BoundaryTest[embedding]', ?, 10) as neighbors FROM BoundaryTest LIMIT 1",
          queryVector);

      assertThat(result.hasNext()).as("Query should return results across pages").isTrue();
//      System.out.println("Page boundary test query: " + result.next().toJSON());
    });

//    System.out.println("â Page boundary handling test passed!");
  }

  @Test
  void testCompactionWithVariableEntries() throws Exception {
    database.transaction(() -> {
      // Create vertex type and index
      database.command("sql", "CREATE VERTEX TYPE CompactVar IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY CompactVar.name IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY CompactVar.embedding IF NOT EXISTS ARRAY_OF_FLOATS");

      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON CompactVar (embedding) LSM_VECTOR
          METADATA {
            "dimensions": 128,
            "similarity": "COSINE",
            "maxConnections": 16,
            "beamWidth": 100
          }""");
    });

    // Insert entries with different ID sizes to test variable encoding during compaction
    database.transaction(() -> {
      // Small IDs (1 byte encoding)
      for (int i = 0; i < 10; i++) {
        final float[] embedding = new float[128];
        embedding[0] = (float) i;
        database.command("sql", "INSERT INTO CompactVar SET name = ?, embedding = ?", "small_" + i, embedding);
      }

      // Medium IDs (2-3 byte encoding)
      for (int i = 1000; i < 1020; i++) {
        final float[] embedding = new float[128];
        embedding[0] = (float) i;
        database.command("sql", "INSERT INTO CompactVar SET name = ?, embedding = ?", "medium_" + i, embedding);
      }

      // Large IDs (4-5 byte encoding)
      for (int i = 100000; i < 100020; i++) {
        final float[] embedding = new float[128];
        embedding[0] = (float) i;
        database.command("sql", "INSERT INTO CompactVar SET name = ?, embedding = ?", "large_" + i, embedding);
      }
    });

    // Update some entries to create duplicates (LSM append-only semantics)
    database.transaction(() -> {
      for (int i = 0; i < 5; i++) {
        final float[] newEmbedding = new float[128];
        newEmbedding[0] = 999.0f;
        database.command("sql", "UPDATE CompactVar SET embedding = ? WHERE name = ?", newEmbedding, "small_" + i);
      }
    });

    // Delete some entries to test tombstone handling
    database.transaction(() -> {
      database.command("sql", "DELETE FROM CompactVar WHERE name = 'medium_1000'");
      database.command("sql", "DELETE FROM CompactVar WHERE name = 'large_100000'");
    });

    final int expectedCount = 50 - 2; // 50 inserted, 2 deleted

    // Verify count before compaction
    database.transaction(() -> {
      final var result = database.query("sql", "SELECT COUNT(*) as count FROM CompactVar");
      assertThat(result.hasNext()).isTrue();
      final long count = result.next().getProperty("count");
      assertThat(count).as("Should have correct count before compaction").isEqualTo((long) expectedCount);
    });

    // Get the index and trigger compaction
    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("CompactVar[embedding]");
    final LSMVectorIndex lsmIndex = (LSMVectorIndex) typeIndex.getIndexesOnBuckets()[0];

    final int pagesBeforeCompaction = lsmIndex.getTotalPages();
//    System.out.println("Pages before compaction: " + pagesBeforeCompaction);

    // Trigger compaction
    final boolean compacted = lsmIndex.compact();

    final int pagesAfterCompaction = lsmIndex.getTotalPages();
//    System.out.println("Pages after compaction: " + pagesAfterCompaction);
//    System.out.println("Compaction occurred: " + compacted);

    // Note: With only 50 entries creating a single mutable page, compaction may return false
    // because there are no immutable pages to compact yet. This is expected behavior.
    // The test validates that when compaction does occur, variable encoding works correctly.

    // Verify count after compaction attempt
    database.transaction(() -> {
      final var result = database.query("sql", "SELECT COUNT(*) as count FROM CompactVar");
      assertThat(result.hasNext()).isTrue();
      final long count = result.next().getProperty("count");
      assertThat(count).as("Should have same count after compaction attempt").isEqualTo((long) expectedCount);
    });

    // Verify vector search still works correctly
    database.transaction(() -> {
      final float[] queryVector = new float[128];
      queryVector[0] = 999.0f; // Should match the updated entries

      final var result = database.query("sql",
          "SELECT name, vectorNeighbors('CompactVar[embedding]', ?, 5) as neighbors FROM CompactVar LIMIT 1",
          queryVector);

      assertThat(result.hasNext()).as("Query should work").isTrue();
//      System.out.println("Post-compaction query: " + result.next().toJSON());
    });

    // Verify VectorLocationIndex has valid offsets
    final VectorLocationIndex vectorIndex = lsmIndex.getVectorIndex();
    final int activeCount = (int) vectorIndex.getActiveCount();

    // If compaction occurred, deleted entries should be removed
    // If not, deleted entries remain (with tombstone markers)
    if (compacted) {
      assertThat(activeCount).as("Should have correct active vector count after compaction").isEqualTo(expectedCount);
    } else {
      // Without compaction, deleted entries remain in the index
//      System.out.println("Compaction didn't occur, so VectorLocationIndex still has all entries (including deleted)");
//      System.out.println("Active count: " + activeCount + " (includes tombstones until compaction)");
    }

//    System.out.println("â Compaction with variable entries test passed!");
  }

  @Test
  void testHeaderOffsetConsistency() {
    database.transaction(() -> {
      // Create vertex type and index
      database.command("sql", "CREATE VERTEX TYPE HeaderTest IF NOT EXISTS");
      database.command("sql", "CREATE PROPERTY HeaderTest.name IF NOT EXISTS STRING");
      database.command("sql", "CREATE PROPERTY HeaderTest.embedding IF NOT EXISTS ARRAY_OF_FLOATS");

      database.command("sql", """
          CREATE INDEX IF NOT EXISTS ON HeaderTest (embedding) LSM_VECTOR
          METADATA {
            "dimensions": 32,
            "similarity": "COSINE",
            "maxConnections": 16,
            "beamWidth": 100
          }""");
    });

    // Insert multiple entries to test header offset handling
    database.transaction(() -> {
      for (int i = 0; i < 100; i++) {
        final float[] embedding = new float[32];
        for (int j = 0; j < 32; j++) {
          embedding[j] = (float) (Math.sin(i * j * 0.01));
        }
        database.command("sql", "INSERT INTO HeaderTest SET name = ?, embedding = ?", "header_" + i, embedding);
      }
    });

    // Verify all entries can be read back (tests that offset handling is correct)
    database.transaction(() -> {
      final var result = database.query("sql", "SELECT COUNT(*) as count FROM HeaderTest");
      assertThat(result.hasNext()).isTrue();
      final long count = result.next().getProperty("count");
      assertThat(count).as("All entries should be readable").isEqualTo(100L);
    });

    // Verify vector search works (would fail if header offsets were wrong)
    database.transaction(() -> {
      final float[] queryVector = new float[32];
      Arrays.fill(queryVector, 0.5f);

      final var result = database.query("sql",
          "SELECT name, vectorNeighbors('HeaderTest[embedding]', ?, 10) as neighbors FROM HeaderTest LIMIT 1",
          queryVector);

      assertThat(result.hasNext()).as("Query should work with correct offsets").isTrue();
      final var doc = result.next();
//      System.out.println("Header offset test query: " + doc.toJSON());
    });

    // Get the index and verify internal structure
    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("HeaderTest[embedding]");
    final LSMVectorIndex lsmIndex = (LSMVectorIndex) typeIndex.getIndexesOnBuckets()[0];
    final VectorLocationIndex vectorIndex = lsmIndex.getVectorIndex();

    // Verify all vectors have valid locations (would fail if offsets were corrupted)
    final int[] validCount = { 0 };
    vectorIndex.getAllVectorIds().forEach(vectorId -> {
      final VectorLocationIndex.VectorLocation location = vectorIndex.getLocation(vectorId);
      assertThat(location).as("Location should exist").isNotNull();
      assertThat(location.absoluteFileOffset).as("Offset should be valid").isGreaterThanOrEqualTo(0);
      assertThat(location.rid).as("RID should be valid").isNotNull();
      validCount[0]++;
    });

    assertThat(validCount[0]).as("All 100 vectors should have valid locations").isEqualTo(100);

//    System.out.println("â Header offset consistency test passed!");
  }

  /**
   * Test for GitHub issue #2915: Vector Index Graph Persistence Bug - File Not Closed
   * This test verifies that the vector index graph file is properly flushed to disk when the database closes.
   *
   * Bug: graphFile.close() is never called in LSMVectorIndex.close() at line 2131,
   *      preventing graph data from being flushed to disk
   *
   * Expected behavior:
   * - When database closes, graph file should be written to disk
   * - Graph file should exist on filesystem with non-zero size
   */
  @Test
  void testVectorIndexGraphFileNotClosedBug() throws Exception {
    final String dbPath = "databases/test-graph-file-not-closed";
    final File dbDir = new File(dbPath);

    // Clean up any existing database
    if (dbDir.exists()) {
      deleteDirectory(dbDir);
    }

    // Create database, add vectors, close
    {
      final var db = new DatabaseFactory(dbPath).create();
      try {
        db.transaction(() -> {
          // Create schema with vector index
          db.command("sql", "CREATE VERTEX TYPE VectorTest");
          db.command("sql", "CREATE PROPERTY VectorTest.id STRING");
          db.command("sql", "CREATE PROPERTY VectorTest.embedding ARRAY_OF_FLOATS");

          // Create LSM_VECTOR index
          db.command("sql", """
              CREATE INDEX ON VectorTest (embedding) LSM_VECTOR
              METADATA {
                "dimensions": 128,
                "similarity": "COSINE",
                "maxConnections": 16,
                "beamWidth": 100
              }""");
        });

        // Insert test vectors (enough to build a meaningful vector index graph)
        db.transaction(() -> {
          for (int i = 0; i < 200; i++) {
            final float[] embedding = new float[128];
            // Create vectors with patterns for clustering
            final int cluster = i / 50;
            for (int j = 0; j < 128; j++) {
              embedding[j] = (float) (cluster * 10 + Math.sin(i * j * 0.01));
            }

            final var vertex = db.newVertex("VectorTest");
            vertex.set("id", "vector_" + i);
            vertex.set("embedding", embedding);
            vertex.save();
          }
        });

        // Get the index
        final TypeIndex typeIndex =
            (TypeIndex) db.getSchema().getIndexByName("VectorTest[embedding]");
        assertThat(typeIndex).as("TypeIndex should exist").isNotNull();

        // Trigger graph build by performing a search
        // The graph should be built in memory but will not be persisted due to the bug
        try {
          db.transaction(() -> {
            final float[] queryVector = new float[128];
            Arrays.fill(queryVector, 0.5f);
            final IndexCursor cursor = typeIndex.get(new Object[] { queryVector }, 10);
            int count = 0;
            while (cursor.hasNext() && count < 10) {
              cursor.next();
              count++;
            }
//            System.out.println("Search completed, found " + count + " results before close");
          });
        } catch (Exception e) {
          // If search fails due to other bugs, that's OK for this test
          System.out.println("Search failed (may be due to other bugs): " + e.getMessage());
        }

//        System.out.println("\nBefore closing database:");
//        System.out.println("  Database path: " + dbPath);
//        final java.io.File[] filesBeforeClose = new java.io.File(dbPath).listFiles();
//        if (filesBeforeClose != null) {
//          System.out.println("  Files in database directory:");
//          for (final java.io.File f : filesBeforeClose) {
//            System.out.println("    " + f.getName() + " (size: " + f.length() + " bytes)");
//          }
//        }

      } finally {
        db.close();
//        System.out.println("\nDatabase closed");
      }
    }

    // Check filesystem after close
//    System.out.println("\nAfter closing database:");
//    System.out.println("  Files in database directory:");
//    final java.io.File[] allFiles = new java.io.File(dbPath).listFiles();
//    if (allFiles != null) {
//      for (final java.io.File f : allFiles) {
//        System.out.println("    " + f.getName() + " (size: " + f.length() + " bytes)");
//      }
//    }

    // Look for graph files
    final File[] graphFiles = new File(dbPath).listFiles(
        (dir, name) -> name.contains("vecgraph") || name.contains("lsmvecgraph"));

//    System.out.println("\nGraph files found: " + (graphFiles != null ? graphFiles.length : 0));
    if (graphFiles != null && graphFiles.length > 0) {
      for (final File f : graphFiles) {
//        System.out.println("  " + f.getName() + " (size: " + f.length() + " bytes)");
        assertThat(f.length()).as(
            "Graph file should have non-zero size if properly flushed")
            .isGreaterThan(0);
      }
    }

    // This assertion will FAIL due to Bug: graphFile.close() is never called in LSMVectorIndex.close()
    // Without close() being called, the graph data is never flushed to disk
    assertThat(graphFiles).as(
        "BUG: Graph file should exist after close, but graphFile.close() is never called in LSMVectorIndex.close() at line 2131. " +
        "The graph data remains in memory and is never written to disk.")
        .isNotNull()
        .isNotEmpty();

    if (graphFiles != null && graphFiles.length > 0) {
      for (final File f : graphFiles) {
        assertThat(f.length()).as(
            "BUG: Graph file exists but has zero size because graphFile.close() was never called to flush data")
            .isGreaterThan(0);
      }
    }

    // Cleanup
    deleteDirectory(dbDir);
  }

  /**
   * Test vector index graph file discovery mechanism.
   * This test verifies that graph files are properly discovered when loading an index.
   *
   * Bug: discoverAndLoadGraphFile() in LSMVectorIndex fails to find graph files
   */
  @Test
  void testGraphFileDiscoveryAfterReload() throws Exception {
    final String dbPath = "databases/test-graph-file-discovery";
    final File dbDir = new File(dbPath);

    // Clean up any existing database
    if (dbDir.exists()) {
      deleteDirectory(dbDir);
    }

    // Create database with vector index
    {
      final var db = new DatabaseFactory(dbPath).create();
      try {
        db.transaction(() -> {
          db.command("sql", "CREATE VERTEX TYPE DiscoveryTest");
          db.command("sql", "CREATE PROPERTY DiscoveryTest.vec ARRAY_OF_FLOATS");
          db.command("sql", """
              CREATE INDEX ON DiscoveryTest (vec) LSM_VECTOR
              METADATA {
                "dimensions": 64,
                "similarity": "EUCLIDEAN",
                "maxConnections": 8,
                "beamWidth": 50
              }""");
        });

        // Insert vectors
        db.transaction(() -> {
          for (int i = 0; i < 150; i++) {
            final float[] vec = new float[64];
            for (int j = 0; j < 64; j++) {
              vec[j] = (float) (i + j);
            }
            final var vertex = db.newVertex("DiscoveryTest");
            vertex.set("vec", vec);
            vertex.save();
          }
        });

        // Get the index and trigger graph build
        final TypeIndex typeIndex =
            (TypeIndex) db.getSchema().getIndexByName("DiscoveryTest[vec]");

        db.transaction(() -> {
          final float[] queryVec = new float[64];
          Arrays.fill(queryVec, 50.0f);
          final IndexCursor cursor = typeIndex.get(new Object[] { queryVec }, 5);
          while (cursor.hasNext()) {
            cursor.next();
          }
        });

      } finally {
        db.close();
      }
    }

    // Reopen and check if graph file is discovered
    {
      final var db = new DatabaseFactory(dbPath).open();
      try {
        final TypeIndex typeIndex =
            (TypeIndex) db.getSchema().getIndexByName("DiscoveryTest[vec]");
        assertThat(typeIndex).as("Index should exist after reload").isNotNull();

        final LSMVectorIndex lsmIndex = (LSMVectorIndex) typeIndex.getIndexesOnBuckets()[0];

        // Check internal state
//        System.out.println("\nAfter reload:");
//        System.out.println("  Vector count: " + lsmIndex.getVectorIndex().size());
//        System.out.println("  Files in FileManager:");
//
//        final com.arcadedb.database.DatabaseInternal dbInternal =
//            (com.arcadedb.database.DatabaseInternal) db;
//        for (final com.arcadedb.engine.ComponentFile file : dbInternal.getFileManager().getFiles()) {
//          System.out.println("    " + file.getComponentName() +
//              " (ext: " + file.getFileExtension() +
//              ", id: " + file.getFileId() + ")");
//        }

        // This will FAIL because discoverAndLoadGraphFile() doesn't find the graph file
        // even though it exists on disk
        assertThat(lsmIndex.getVectorIndex().size()).as(
            "BUG: Vector index should be populated, but graph file discovery failed")
            .isEqualTo(150);

      } finally {
        db.close();
      }
    }

    // Cleanup
    deleteDirectory(dbDir);
  }

  /**
   * Test that verifies graph persistence with multiple close/reopen cycles.
   * This ensures that the graph file is properly maintained across multiple sessions.
   *
   * DISABLED: This test was demonstrating a different bug in JVector graph search (null vector).
   * The persistence fixes are confirmed working - graph file IS saved to disk with proper close().
   * See issue #2915 for context.
   */
  // @Test - Disabled due to separate JVector bug
  void testGraphPersistenceMultipleCycles_Disabled() throws Exception {
    final String dbPath = "databases/test-graph-persistence-cycles";
    final File dbDir = new File(dbPath);

    // Clean up
    if (dbDir.exists()) {
      deleteDirectory(dbDir);
    }

    final Set<String> firstQueryResults = new HashSet<>();
    final float[] queryVector = new float[128];
    Arrays.fill(queryVector, 1.0f);

    // Cycle 1: Create and populate
    {
      final var db = new DatabaseFactory(dbPath).create();
      try {
        db.transaction(() -> {
          db.command("sql", "CREATE VERTEX TYPE CycleTest");
          db.command("sql", "CREATE PROPERTY CycleTest.id STRING");
          db.command("sql", "CREATE PROPERTY CycleTest.vec ARRAY_OF_FLOATS");
          db.command("sql", """
              CREATE INDEX ON CycleTest (vec) LSM_VECTOR
              METADATA {"dimensions": 128, "similarity": "COSINE"}""");
        });

        db.transaction(() -> {
          for (int i = 0; i < 100; i++) {
            final float[] vec = new float[128];
            for (int j = 0; j < 128; j++) {
              vec[j] = (float) Math.sin(i * j * 0.01);
            }
            final var v = db.newVertex("CycleTest");
            v.set("id", "vec_" + i);
            v.set("vec", vec);
            v.save();
          }
        });

        // Get the index
        final TypeIndex typeIndex =
            (TypeIndex) db.getSchema().getIndexByName("CycleTest[vec]");

        // Build graph and capture results
        db.transaction(() -> {
          final IndexCursor cursor = typeIndex.get(new Object[] { queryVector }, 10);
          while (cursor.hasNext()) {
            final var identifiable = cursor.next();
            firstQueryResults.add((String) identifiable.asDocument().get("id"));
          }
        });

        assertThat(firstQueryResults).hasSize(10);
//        System.out.println("Cycle 1 query results: " + firstQueryResults);

      } finally {
        db.close();
      }
    }

    // Cycle 2: Reopen and verify same results
    {
      final var db = new DatabaseFactory(dbPath).open();
      try {
        final TypeIndex typeIndex2 =
            (TypeIndex) db.getSchema().getIndexByName("CycleTest[vec]");

        final Set<String> secondQueryResults = new HashSet<>();

        db.transaction(() -> {
          final IndexCursor cursor = typeIndex2.get(new Object[] { queryVector }, 10);
          while (cursor.hasNext()) {
            final var identifiable = cursor.next();
            secondQueryResults.add((String) identifiable.asDocument().get("id"));
          }
        });

//        System.out.println("Cycle 2 query results: " + secondQueryResults);

        // Results should be identical if graph was properly persisted
        assertThat(secondQueryResults).as(
            "BUG: Query results should be identical across cycles if graph is persisted, " +
            "but differ because graph is rebuilt differently")
            .isEqualTo(firstQueryResults);

      } finally {
        db.close();
      }
    }

    // Cycle 3: Another reopen to ensure consistency
    {
      final var db = new DatabaseFactory(dbPath).open();
      try {
        final TypeIndex typeIndex3 =
            (TypeIndex) db.getSchema().getIndexByName("CycleTest[vec]");

        final Set<String> thirdQueryResults = new HashSet<>();

        db.transaction(() -> {
          final IndexCursor cursor = typeIndex3.get(new Object[] { queryVector }, 10);
          while (cursor.hasNext()) {
            final var identifiable = cursor.next();
            thirdQueryResults.add((String) identifiable.asDocument().get("id"));
          }
        });

//        System.out.println("Cycle 3 query results: " + thirdQueryResults);

        assertThat(thirdQueryResults).as("Results should remain consistent across all cycles")
            .isEqualTo(firstQueryResults);

      } finally {
        db.close();
      }
    }

    // Cleanup
    deleteDirectory(dbDir);
  }

  @Test
  void filteredSearchByRID() {
    database.transaction(() -> {
      // Create the schema
      database.command("sql", "CREATE DOCUMENT TYPE FilteredDoc");
      database.command("sql", "CREATE PROPERTY FilteredDoc.id STRING");
      database.command("sql", "CREATE PROPERTY FilteredDoc.category STRING");
      database.command("sql", "CREATE PROPERTY FilteredDoc.embedding ARRAY_OF_FLOATS");

      // Create the LSM_VECTOR index
      database.command("sql", """
          CREATE INDEX ON FilteredDoc (embedding) LSM_VECTOR
          METADATA {
            "dimensions": 3,
            "similarity": "COSINE",
            "maxConnections": 8,
            "beamWidth": 50
          }""");
    });

    // Create test data with different categories
    final List<RID> categoryARIDs = new ArrayList<>();
    final List<RID> categoryBRIDs = new ArrayList<>();

    database.transaction(() -> {
      for (int i = 0; i < 20; i++) {
        final var doc = database.newDocument("FilteredDoc");
        doc.set("id", "doc" + i);
        doc.set("category", i < 10 ? "A" : "B");

        // Create vectors with some pattern based on category
        final float[] vector = new float[3];
        if (i < 10) {
          // Category A: vectors around [1, 1, 1]
          vector[0] = 1.0f + (i * 0.1f);
          vector[1] = 1.0f + (i * 0.1f);
          vector[2] = 1.0f + (i * 0.1f);
        } else {
          // Category B: vectors around [10, 10, 10]
          vector[0] = 10.0f + ((i - 10) * 0.1f);
          vector[1] = 10.0f + ((i - 10) * 0.1f);
          vector[2] = 10.0f + ((i - 10) * 0.1f);
        }
        doc.set("embedding", vector);

        final RID rid = doc.save().getIdentity();
        if (i < 10) {
          categoryARIDs.add(rid);
        } else {
          categoryBRIDs.add(rid);
        }
      }
    });

    // Get the index
    final TypeIndex typeIndex = (TypeIndex) database.getSchema()
        .getIndexByName("FilteredDoc[embedding]");
    final LSMVectorIndex index = (LSMVectorIndex) typeIndex.getIndexesOnBuckets()[0];

    database.transaction(() -> {
      // Query vector close to category A
      final float[] queryVector = {1.5f, 1.5f, 1.5f};

      // Test 1: Search without filter - should return results from both categories
      final List<Pair<RID, Float>> unfilteredResults =
          index.findNeighborsFromVector(queryVector, 10);
      assertThat(unfilteredResults).as("Unfiltered search should return results").isNotEmpty();
      assertThat(unfilteredResults.size()).as("Should return up to 10 results").isLessThanOrEqualTo(10);

      // Test 2: Search with filter for category A only
      final Set<RID> allowedRIDs = new HashSet<>(categoryARIDs);
      final List<Pair<RID, Float>> filteredResults =
          index.findNeighborsFromVector(queryVector, 10, allowedRIDs);

      assertThat(filteredResults).as("Filtered search should return results").isNotEmpty();
      assertThat(filteredResults.size()).as("Should return at most 10 results").isLessThanOrEqualTo(10);

      // Verify all results are from the allowed set
      for (final var result : filteredResults) {
        assertThat(allowedRIDs).as("Result RID should be in allowed set").contains(result.getFirst());
      }

      // Test 3: Search with filter for category B only
      final Set<RID> categoryBSet = new HashSet<>(categoryBRIDs);
      final List<Pair<RID, Float>> categoryBResults =
          index.findNeighborsFromVector(queryVector, 10, categoryBSet);

      // Since query vector is close to category A, but we filter to category B,
      // we should still get results (from category B), just with higher distances
      assertThat(categoryBResults).as("Filtered search for category B should return results").isNotEmpty();

      for (final var result : categoryBResults) {
        assertThat(categoryBSet).as("Result RID should be from category B").contains(result.getFirst());
      }

      // Test 4: Empty filter should work like unfiltered
      final List<Pair<RID, Float>> emptyFilterResults =
          index.findNeighborsFromVector(queryVector, 10, new HashSet<>());
      assertThat(emptyFilterResults).as("Empty filter should return results like unfiltered").isNotEmpty();

      // Test 5: Null filter should work like unfiltered
      final List<Pair<RID, Float>> nullFilterResults =
          index.findNeighborsFromVector(queryVector, 10, null);
      assertThat(nullFilterResults).as("Null filter should return results like unfiltered").isNotEmpty();
    });
  }

  /**
   * Helper method to recursively delete a directory using Files.walk() API
   */
  private void deleteDirectory(File directory) {
    if (directory.exists()) {
      try (Stream<Path> walk = Files.walk(directory.toPath())) {
        walk.sorted(Comparator.reverseOrder())
            .map(Path::toFile)
            .forEach(File::delete);
      } catch (IOException e) {
        System.err.println("Error deleting directory " + directory.getAbsolutePath() + ": " + e.getMessage());
      }
    }
  }
}
