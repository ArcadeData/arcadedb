# Docker build for ArcadeDB Python package
# Builds arcadedb-embedded package with bundled JRE (no external Java required)
# Excludes JARs listed in jar_exclusions.txt to optimize size
#
# REQUIRED BUILD ARGS (passed from build.sh):
#   PYTHON_VERSION - Python version for wheel (e.g., 3.11, 3.12, 3.13, 3.14)
#   PACKAGE_NAME - always: arcadedb-embedded
#   PACKAGE_DESCRIPTION - package description
#   TARGET_PLATFORM - target platform for JRE (e.g., linux-x64, linux-arm64, darwin-x64)

ARG PYTHON_VERSION=3.12
ARG PACKAGE_NAME=arcadedb-embedded
ARG PACKAGE_DESCRIPTION="ArcadeDB embedded multi-model database with bundled JRE - no Java installation required"
ARG ARCADEDB_TAG
ARG TARGET_PLATFORM=linux-x64
# When set to 1, prefer jars provided in bindings/python/local-jars/lib from the build context.
# If no local jars are present, the build fails fast to avoid silently falling back.
ARG USE_LOCAL_JARS=0

# Stage 1: Use prebuilt ArcadeDB image to obtain compiled JARs
# JARs are filtered based on jar_exclusions.txt in later stages
FROM arcadedata/arcadedb:${ARCADEDB_TAG} AS java-builder

# nothing to do here; jars will be copied from /home/arcadedb/lib in the python-builder stage

# Stage 2: Build minimal JRE with jlink
FROM amazoncorretto:25 AS jre-builder

# Install required tooling (findutils for jar exclusion, binutils for jlink)
RUN yum -y install findutils binutils && yum clean all

ARG TARGET_PLATFORM
ARG USE_LOCAL_JARS

WORKDIR /build

# Stash upstream jars from the ArcadeDB image
RUN mkdir -p /build/upstream-jars /build/jars /build/local-jars
COPY --from=java-builder /home/arcadedb/lib /build/upstream-jars/

# Optionally bring in locally built jars from the repo (bindings/python/local-jars/lib)
COPY bindings/python/local-jars/lib/ /build/local-jars/

# Select jar source: local when requested and available; otherwise fall back to upstream image
RUN if [ "$USE_LOCAL_JARS" = "1" ]; then \
            if [ -d /build/local-jars ] && [ "$(ls -1 /build/local-jars | wc -l)" -gt 0 ]; then \
                echo "üì¶ Using local jars from build context" && cp /build/local-jars/* /build/jars/; \
            else \
                echo "‚ùå USE_LOCAL_JARS=1 but no jars found in bindings/python/local-jars/lib" && exit 1; \
            fi; \
        else \
            echo "üì¶ Using ArcadeDB image jars" && cp /build/upstream-jars/* /build/jars/; \
        fi

# Copy JAR exclusion list
COPY bindings/python/jar_exclusions.txt /build/jar_exclusions.txt

# Remove excluded JARs based on jar_exclusions.txt
RUN echo "üóëÔ∏è  Removing excluded JARs..." && \
    echo "üìã JAR count before exclusion: $(ls -1 /build/jars/*.jar | wc -l)" && \
    while IFS= read -r pattern; do \
        # Skip empty lines and comments
        if [ -n "$pattern" ] && [ "${pattern#\#}" = "$pattern" ]; then \
            echo "   Processing pattern: $pattern" && \
            # Use find and delete to show what's being removed
            find /build/jars -name "$pattern" -type f -exec echo "   - Removing: {}" \; -delete; \
        fi \
    done < /build/jar_exclusions.txt && \
    echo "üìã JAR count after exclusion: $(ls -1 /build/jars/*.jar | wc -l)"

# Build minimal JRE with jlink
# Automatically detect modules with jdeps, plus ensure jdk.zipfs (for JPype) and jdk.unsupported are included
# We exclude jboss/wildfly JARs because they have broken module descriptors that fail analysis.
# We also do NOT provide a classpath, forcing jdeps to ignore all missing dependencies (intra-jar or external).
RUN echo "üîç Analyzing JARs with jdeps..." && \
    DETECTED_MODULES=$(find /build/jars -name "*.jar" | grep -v "jboss" | grep -v "wildfly" | grep -v "smallrye" | xargs jdeps --print-module-deps --ignore-missing-deps --multi-release 25 | grep -v "Warning" | tr ',' '\n' | grep -v "Warning" | grep -v ":" | grep -v "/" | sort -u | paste -sd "," -) && \
    REQUIRED_MODULES="${DETECTED_MODULES},jdk.zipfs,jdk.unsupported" && \
    echo "üî® Building minimal JRE for platform: ${TARGET_PLATFORM}" && \
    echo "üì¶ Detected modules: ${DETECTED_MODULES}" && \
    echo "üì¶ Final modules list: ${REQUIRED_MODULES}" && \
    echo "üì¶ Required modules:" && \
    echo "$REQUIRED_MODULES" | tr ',' '\n' | sed 's/^/   - /' && \
    JMODS_DIR="${JAVA_HOME}/jmods" && \
    if [ ! -d "$JMODS_DIR" ]; then JMODS_DIR="${JAVA_HOME}/lib/jmods"; fi && \
    if [ -d "$JMODS_DIR" ]; then \
        echo "" ; \
        echo "üî® Running jlink..." ; \
        jlink \
            --module-path "$JMODS_DIR" \
            --add-modules "${REQUIRED_MODULES}" \
            --ignore-signing-information \
            --strip-debug \
            --no-man-pages \
            --no-header-files \
            --compress zip-9 \
            --output /build/jre ; \
        echo "" ; \
        echo "‚úÖ JRE build complete!" ; \
    else \
        echo "‚ö†Ô∏è  jmods directory not found under ${JAVA_HOME}. Falling back to copying full JDK runtime." ; \
        mkdir -p /build/jre ; \
        cp -a ${JAVA_HOME}/bin /build/jre/bin ; \
        cp -a ${JAVA_HOME}/lib /build/jre/lib ; \
        cp -a ${JAVA_HOME}/conf /build/jre/conf || true ; \
        cp -a ${JAVA_HOME}/release /build/jre/release || true ; \
        echo "‚úÖ Fallback JRE created from full JDK runtime." ; \
    fi && \
    echo "" && \
    JRE_SIZE=$(du -sh /build/jre | cut -f1) && \
    echo "üìä JRE size: $JRE_SIZE" && \
    echo "" && \
    echo "üß™ Testing JRE..." && \
    /build/jre/bin/java -version && \
    echo "" && \
    echo "‚úÖ JRE is functional!" && \
    if [ ! -f /build/jre/bin/java ]; then \
        echo "‚ùå ERROR: JRE build failed - java executable not found!"; \
        exit 1; \
    fi

# Stage 3: Build Python wheel

FROM python:${PYTHON_VERSION}-slim AS python-builder

# Install minimal build tooling (no external JDK needed; we use bundled JRE)
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    file \
    patchelf \
    && rm -rf /var/lib/apt/lists/*

# Install uv for faster, deterministic installs
ENV PATH="/root/.cargo/bin:/root/.local/bin:${PATH}"
RUN curl -LsSf https://astral.sh/uv/install.sh | sh && uv --version

WORKDIR /build

# Copy filtered JARs from jre-builder (already has exclusions applied)
RUN mkdir -p /build/jars
COPY --from=jre-builder /build/jars /build/jars/

# Copy JRE from jre-builder stage
COPY --from=jre-builder /build/jre /build/jre/

# Copy Python bindings source
COPY bindings/python/src ./src
COPY bindings/python/tests ./tests
COPY bindings/python/setup.py .
COPY bindings/python/setup_jars.py .
COPY bindings/python/extract_version.py .
COPY bindings/python/write_version.py .
COPY bindings/python/jar_exclusions.txt .
COPY bindings/python/pyproject.toml ./
COPY bindings/python/README.md ./

# Also copy repository pom.xml so extract_version.py can read it
COPY ../../pom.xml /arcadedb/pom.xml

# Install Python build dependencies
RUN uv pip install --system build wheel setuptools jpype1 auditwheel

# Re-declare build args for this stage (required after FROM)
ARG PACKAGE_NAME
ARG PACKAGE_DESCRIPTION
ARG ARCADEDB_TAG
ARG TARGET_PLATFORM

# Extract version and copy JARs (including JRE)
# Version is passed from build.sh via ARCADEDB_TAG build arg
RUN echo "üìå Package name: ${PACKAGE_NAME}" && \
    echo "üìå Docker tag used: ${ARCADEDB_TAG}" && \
    echo "üìå Target platform: ${TARGET_PLATFORM}" && \
    python3 write_version.py /arcadedb/pom.xml && \
    python3 setup_jars.py && \
    echo "üì¶ JAR files and JRE copied"

# Build the wheel
# Extract PEP 440 version from pom.xml or use BUILD_VERSION if provided
ARG BUILD_VERSION=""
RUN if [ -n "${BUILD_VERSION}" ]; then \
        export ARCADEDB_VERSION="${BUILD_VERSION}"; \
        echo "üìå Using provided version: ${ARCADEDB_VERSION}"; \
    else \
        export ARCADEDB_VERSION=$(python3 extract_version.py --format=pep440 /arcadedb/pom.xml); \
        echo "üì¶ Extracted version from pom.xml: ${ARCADEDB_VERSION}"; \
    fi && \
    echo "üì¶ Python package version: ${ARCADEDB_VERSION}" && \
    echo "üì¶ Package name: ${PACKAGE_NAME}" && \
    echo "üì¶ Package description: ${PACKAGE_DESCRIPTION}" && \
    sed -i 's|^name = .*|name = "'"${PACKAGE_NAME}"'"|' pyproject.toml && \
    sed -i 's|^version = .*|version = "'"${ARCADEDB_VERSION}"'"|' pyproject.toml && \
    sed -i 's|^description = .*|description = "'"${PACKAGE_DESCRIPTION}"'"|' pyproject.toml && \
    if echo "${TARGET_PLATFORM}" | grep -q '^linux-'; then \
        if [ "${TARGET_PLATFORM}" = "linux-x64" ]; then \
            WHEEL_PLAT="manylinux_2_35_x86_64"; \
        elif [ "${TARGET_PLATFORM}" = "linux-arm64" ]; then \
            WHEEL_PLAT="manylinux_2_35_aarch64"; \
        else \
            WHEEL_PLAT=""; \
        fi; \
        if [ -n "${WHEEL_PLAT}" ]; then \
            echo "üè∑Ô∏è  Building wheel with platform tag: ${WHEEL_PLAT}"; \
            python3 -m build --wheel --config-setting=--build-option=--plat-name=${WHEEL_PLAT}; \
        else \
            python3 -m build --wheel; \
        fi; \
    else \
        python3 -m build --wheel; \
    fi && \
    if echo "${TARGET_PLATFORM}" | grep -q '^linux-'; then \
        if [ -n "${WHEEL_PLAT}" ] && ! ls /build/dist/*${WHEEL_PLAT}*.whl 1> /dev/null 2>&1; then \
            echo "‚ùå Expected manylinux wheel (${WHEEL_PLAT}) not found"; \
            ls -lh /build/dist; \
            exit 1; \
        fi; \
    fi && \
    echo "‚úÖ Wheel built successfully!" && \
    ls -lh dist/

# Stage 4: Export stage (for extracting wheels)
FROM python-builder AS export
# This stage exists to provide a stable location for wheel extraction
# The dist directory is preserved from python-builder stage

# Stage 5: Test the built wheel
FROM python:${PYTHON_VERSION}-slim AS tester

# No external JRE needed in tester; the wheel includes a bundled JRE
# Keep build-essential in case pytest or wheels need compilation on some platforms
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install uv for faster, deterministic installs
ENV PATH="/root/.cargo/bin:/root/.local/bin:${PATH}"
RUN curl -LsSf https://astral.sh/uv/install.sh | sh && uv --version

WORKDIR /test

# Copy the wheel from builder
COPY --from=python-builder /build/dist/*.whl /tmp/

# Install the wheel and test dependencies
RUN uv pip install --system /tmp/*.whl pytest pytest-cov

# Copy tests
COPY --from=python-builder /build/tests ./tests/

# Create a quick installation test
RUN echo '#!/usr/bin/env python3\n\
import arcadedb_embedded as arcadedb\n\
import tempfile\n\
import shutil\n\
import os\n\
\n\
print("üéÆ Testing ArcadeDB Python bindings...")\n\
print(f"üì¶ Version: {arcadedb.__version__}")\n\
\n\
temp_dir = tempfile.mkdtemp()\n\
db_path = os.path.join(temp_dir, "test_db")\n\
\n\
try:\n\
    with arcadedb.create_database(db_path) as db:\n\
        print("‚úÖ Database created")\n\
        \n\
        with db.transaction():\n\
            db.command("sql", "CREATE DOCUMENT TYPE TestDoc")\n\
            db.command("sql", "INSERT INTO TestDoc SET name = '\''docker_test'\'', value = 123")\n\
        print("‚úÖ Transaction committed")\n\
        \n\
        result = db.query("sql", "SELECT FROM TestDoc")\n\
        for record in result:\n\
            print(f"‚úÖ Query result: {record.get('\''name'\'')} = {record.get('\''value'\'')}")\n\
    \n\
    print("üéâ All tests passed!")\n\
finally:\n\
    if os.path.exists(temp_dir):\n\
        shutil.rmtree(temp_dir)\n\
' > /test/test_install.py && chmod +x /test/test_install.py

# Run the installation test
RUN python3 /test/test_install.py

CMD ["python3", "/test/test_install.py"]
